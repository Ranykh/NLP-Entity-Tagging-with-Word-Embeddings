{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87456f06",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import gensim\n",
    "from gensim import downloader\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880e5ed2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp39-cp39-macosx_10_9_x86_64.whl (24.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 24.1 MB 8.0 MB/s eta 0:00:01     |███████████████████████████▌    | 20.7 MB 6.3 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-7.0.1-py3-none-any.whl (60 kB)\n",
      "\u001B[K     |████████████████████████████████| 60 kB 6.3 MB/s  eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.18.5 in /Users/ranykhirbawi/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/ranykhirbawi/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: wrapt in /Users/ranykhirbawi/opt/anaconda3/lib/python3.9/site-packages (from smart-open>=1.8.1->gensim) (1.12.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-7.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "686c705a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\"\"\"[('paulwalk', 0), ('it', 0), ('s', 0), ('the', 0), ('view', 0), ('from', 0), ('where', 0), ('i', 0), ('m', 0), ('living', 0), ('for', 0), ('two', 0), ('weeks', 0), ('empire', 1), ('state', 1), ('building', 1), ('esb', 1), ('pretty', 0), ('bad', 0), ('storm', 0), ('here', 0), ('last', 0), ('evening', 0)]\"\"\"\n",
    "def extract_sentences_with_tags(file_path, untagged):\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "#     # Define a translation table to remove punctuation\n",
    "#     punctuation_marks = {'.', ',', '=', '...', '!', '?', ':', ';', '&lt;', '&gt;', '-', '(', ')', '[', ']', '{', '}',\n",
    "#                          '\"', \"'\"}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                # Split the line into word and label\n",
    "                if untagged:\n",
    "                    parts = [line, '_']\n",
    "                else:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) != 2:\n",
    "                        continue\n",
    "                    \n",
    "\n",
    "                word, tag = parts\n",
    "                binary_tag = 0 if tag == 'O' else 1\n",
    "                word = word.lower()  # Convert word to lowercase\n",
    "                \n",
    "#                 if word in punctuation_marks:\n",
    "#                     continue  # Skip words starting with @ or #, and punctuation marks\n",
    "                    \n",
    "                if not any(char.isalpha() for char in word):\n",
    "                    continue\n",
    "                \n",
    "                elif current_sentence and (word.startswith(\"'\") or word == \"n't\"):  # Handle contractions and negations\n",
    "                    # Combine with previous word and keep the most specific tag (non-'O' if present)\n",
    "                    prev_word, prev_tag = current_sentence[-1]\n",
    "                    current_sentence[-1] = (prev_word + word, prev_tag if prev_tag != 0 else binary_tag)\n",
    "                else:\n",
    "                    current_sentence.append((word, binary_tag))\n",
    "#                 if untagged:\n",
    "#                     current_sentence.append(word)\n",
    "#                 else:\n",
    "#                     try:\n",
    "#                         label = parts[1]\n",
    "#                     except: continue\n",
    "                    # Convert label to binary (0 for 'O', 1 otherwise)\n",
    "#                 i\n",
    "    \n",
    "#                     binary_label = 0 if tag == 'O' else 1\n",
    "#                     current_sentence.append((word, binary_label))\n",
    "            else:\n",
    "                if current_sentence:  # Only add non-empty sentences\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "                    \n",
    "        # Add the last sentence if file doesn't end with a blank line\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5019fe5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def sentence_to_embedding(sentence, model, embedding_dim=200):\n",
    "    # Initialize an empty list to hold the embeddings\n",
    "    embedding_matrix = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in model.key_to_index:\n",
    "            embedding_matrix.append(model[word])\n",
    "        else:\n",
    "            if len(embedding_matrix) > 3:\n",
    "                former_vectors = [vec for vec in embedding_matrix[-3:]]\n",
    "                vec = np.mean(former_vectors, axis=0)\n",
    "            else:\n",
    "                former_vectors = [vec for vec in embedding_matrix]\n",
    "                for i in range(2 - len(embedding_matrix)):\n",
    "                    former_vectors.append(np.random.rand(model.vector_size))\n",
    "                vec = np.mean(former_vectors, axis=0)\n",
    "            embedding_matrix.append(vec)\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "# def sentence_to_embedding(sentence, model, embedding_dim=300, window_size=2):\n",
    "#     # Initialize an empty list to hold the embeddings\n",
    "#     embedding_matrix = []\n",
    "    \n",
    "#     for i, word in enumerate(sentence):\n",
    "#         if word in model.key_to_index:\n",
    "#             embedding_matrix.append(model[word])\n",
    "#         else:\n",
    "#             # If the word is not in the vocabulary, try to use the context\n",
    "#             context_embeddings = []\n",
    "#             # Look before the current word\n",
    "#             for j in range(max(0, i-window_size), i):\n",
    "#                 if sentence[j] in model.key_to_index:\n",
    "#                     context_embeddings.append(model[sentence[j]])\n",
    "#             # Look after the current word\n",
    "#             for j in range(i+1, min(len(sentence), i+window_size+1)):\n",
    "#                 if sentence[j] in model.key_to_index:\n",
    "#                     context_embeddings.append(model[sentence[j]])\n",
    "            \n",
    "#             # If we found any context words, average their embeddings\n",
    "#             if context_embeddings:\n",
    "#                 avg_embedding = np.mean(context_embeddings, axis=0)\n",
    "#                 embedding_matrix.append(avg_embedding)\n",
    "#             else:\n",
    "#                 # If no context words found or all context words are also OOV, use a zero vector\n",
    "#                 embedding_matrix.append(np.zeros(embedding_dim))\n",
    "    \n",
    "#     return np.array(embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c4761d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_sentences(sen_with_tags):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sentence in sen_with_tags:\n",
    "        X.append([word for word, tag in sentence])\n",
    "        y.append([tag for word, tag in sentence])\n",
    "    return X, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06638219",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad_embeddings(embeddings, max_len, embedding_dim=200):\n",
    "    padded_embeddings = []\n",
    "    for sentence in embeddings:\n",
    "        # Calculate how many zeros need to be added\n",
    "        padding_size = max_len - len(sentence)\n",
    "        if padding_size > 0:\n",
    "            # Pad the sentence with zero vectors at the end\n",
    "            sentence = np.vstack((sentence, np.zeros((padding_size, embedding_dim))))\n",
    "        padded_embeddings.append(sentence)\n",
    "    return np.array(padded_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea01d15e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad_labels(labels, max_len, padding_label):\n",
    "    padded_labels = []\n",
    "    for label_seq in labels:\n",
    "        # Calculate how many padding labels need to be added\n",
    "        padding_size = max_len - len(label_seq)\n",
    "        if padding_size > 0:\n",
    "            # Extend the label sequence with the padding label\n",
    "            label_seq.extend([padding_label] * padding_size)\n",
    "        padded_labels.append(label_seq)\n",
    "    return padded_labels\n",
    "\n",
    "# Assuming `labels` is a list of lists where each sublist contains labels for a sentence\n",
    "# and `max_len` is the maximum sentence length determined previously\n",
    "# padded_labels = pad_labels(labels, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "675c43d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-------------------------------------------------] 3.1% 23.4/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====----------------------------------------------] 8.4% 63.4/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======--------------------------------------------] 13.9% 105.7/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 19.0% 143.8/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============--------------------------------------] 24.2% 183.6/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============------------------------------------] 29.7% 225.6/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================---------------------------------] 35.7% 271.0/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================------------------------------] 40.9% 310.5/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================---------------------------] 46.2% 350.3/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================-------------------------] 51.7% 391.9/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================----------------------] 57.0% 432.6/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 61.7% 468.0/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================-----------------] 67.4% 511.0/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================--------------] 72.3% 548.8/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================------------] 77.2% 585.8/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================================---------] 83.7% 635.1/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================------] 89.7% 680.7/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================--] 96.7% 733.3/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 758.5/758.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import downloader\n",
    "\n",
    "# we want word2vec representation\n",
    "# WORD_2_VEC_PATH = 'word2vec-google-news-300'\n",
    "# word2vec_model = downloader.load(WORD_2_VEC_PATH)\n",
    "GLOVE_PATH = 'glove-twitter-200'\n",
    "# word2vec_model = downloader.load(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "105aff4e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class FF_NN(nn.Module):\n",
    "\n",
    "    def __init__(self, vec_dim, num_classes, hidden_dim=100):\n",
    "        super(FF_NN, self).__init__()\n",
    "        self.ignore_index = -100\n",
    "        self.first_layer = nn.Linear(vec_dim, hidden_dim)\n",
    "        self.second_layer = nn.Linear(hidden_dim, num_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        x = self.first_layer(input_ids)\n",
    "        x = self.activation(x)\n",
    "        x = self.second_layer(x)  # Logits: [batch_size, sequence_length, num_classes]\n",
    "\n",
    "        if labels is not None:\n",
    "            # Flatten logits and labels to align with CrossEntropyLoss expectations\n",
    "            logits_flat = x.view(-1, x.size(-1))  # [batch_size * sequence_length, num_classes]\n",
    "            labels_flat = labels.view(-1)  # [batch_size * sequence_length]\n",
    "            loss = self.loss(logits_flat, labels_flat)\n",
    "            return logits_flat, loss\n",
    "\n",
    "        # If no labels provided, return logits in their original shape for inference\n",
    "        return x, None\n",
    "\n",
    "#     def forward(self, input_ids, labels=None):\n",
    "#         x = self.first_layer(input_ids)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.second_layer(x)\n",
    "\n",
    "#         # Check if labels were provided and are not already a tensor\n",
    "#         if labels is not None:\n",
    "#             if not isinstance(labels, torch.Tensor):\n",
    "#                 # Convert labels to a tensor if they are not already\n",
    "#                 # Ensure labels are on the same device as the model\n",
    "#                 labels = torch.tensor(labels, dtype=torch.long, device=x.device)\n",
    "\n",
    "#             # Compute the loss\n",
    "#             loss = self.loss(x, labels)\n",
    "#             return x, loss\n",
    "\n",
    "#         # If labels are not provided, just return the logits\n",
    "#         return x, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "264fa00a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NERDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, vector_type, untagged):\n",
    "        self.file_path = file_path\n",
    "        data = extract_sentences_with_tags(file_path, untagged)\n",
    "        \n",
    "        # Assuming `sentences` is your list of sentences (as lists of words) and `labels` is your list of labels\n",
    "        sentences, labels = split_sentences(data)\n",
    "\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        \n",
    "        all_labels = [label for sublist in self.labels for label in sublist]\n",
    "        unique_labels = sorted(set(all_labels))\n",
    "        self.tags_to_idx = {tag: idx for idx, tag in enumerate(unique_labels)}\n",
    "        self.idx_to_tag = {idx: tag for tag, idx in self.tags_to_idx.items()}\n",
    "        \n",
    "        self.labels_idx = [[self.tags_to_idx[label] for label in label_seq] for label_seq in labels]\n",
    "\n",
    "        \n",
    "        self.vector_type = vector_type\n",
    "        \n",
    "        \n",
    "        if vector_type == 'w2v':\n",
    "            model = downloader.load(WORD_2_VEC_PATH)\n",
    "        elif vector_type == 'glove':\n",
    "            model = downloader.load(GLOVE_PATH)\n",
    "        else:\n",
    "            raise KeyError(f\"{vector_type} is not a supported vector type\")\n",
    "            \n",
    "        # Process sentences to embeddings\n",
    "        self.tokenized_sen = [sentence_to_embedding(sen, model, embedding_dim=300) for sen in self.sentences]\n",
    "        self.max_len = max(len(sen) for sen in self.tokenized_sen)\n",
    "\n",
    "        # Apply padding\n",
    "        self.tokenized_sen = pad_embeddings(self.tokenized_sen, self.max_len)\n",
    "        self.ignore_index = -100\n",
    "        self.labels_idx = pad_labels(self.labels_idx, self.max_len, self.ignore_index)\n",
    "        \n",
    "        self.vector_dim = self.tokenized_sen.shape[-1]\n",
    "\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        cur_sen = self.tokenized_sen[item]\n",
    "        cur_labels = self.labels_idx[item]\n",
    "\n",
    "        cur_sen = torch.FloatTensor(cur_sen)\n",
    "        cur_labels = torch.LongTensor(cur_labels)\n",
    "\n",
    "        return {\"input_ids\": cur_sen, \"labels\": cur_labels}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "315ab26b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# def train(model, data_sets, optimizer, num_epochs: int, batch_size=16):\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "#     criterion = torch.nn.CrossEntropyLoss(ignore_index=model.ignore_index)\n",
    "\n",
    "#     data_loaders = {\n",
    "#         \"train\": DataLoader(data_sets[\"train\"], batch_size=batch_size, shuffle=True),\n",
    "#         \"test\": DataLoader(data_sets[\"test\"], batch_size=batch_size, shuffle=False)\n",
    "#     }\n",
    "\n",
    "#     best_acc = 0.0\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "#         print('-' * 10)\n",
    "\n",
    "#         for phase in ['train', 'test']:\n",
    "#             model.train() if phase == 'train' else model.eval()\n",
    "\n",
    "#             running_loss = 0.0\n",
    "#             true_labels = []\n",
    "#             predictions = []\n",
    "\n",
    "#             for batch in data_loaders[phase]:\n",
    "#                 inputs = batch[\"input_ids\"].to(device)\n",
    "#                 labels = batch[\"labels\"].to(device) if phase == 'train' else None\n",
    "                \n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 with torch.set_grad_enabled(phase == 'train'):\n",
    "#                     # Directly use the model's output for logits and loss\n",
    "#                     outputs, loss = model(inputs, labels) if labels is not None else model(inputs)\n",
    "\n",
    "#                     if loss is None and labels is not None:\n",
    "#                         # If the model doesn't calculate loss (e.g., during inference or specific conditions),\n",
    "#                         # we need to calculate it manually for the training phase.\n",
    "#                         outputs_flat = outputs.view(-1, model.num_classes)\n",
    "#                         labels_flat = labels.view(-1)\n",
    "#                         loss = criterion(outputs_flat, labels_flat)\n",
    "\n",
    "#                     if phase == 'train' and loss is not None:\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "\n",
    "#                     # For accuracy calculation, ensure logits are in the correct shape\n",
    "#                     # and only calculate for non-None labels\n",
    "#                     if labels is not None:\n",
    "#                         logits = outputs if loss is None else outputs.view(-1, model.num_classes)\n",
    "#                         _, preds = torch.max(logits, dim=1)\n",
    "#                         labels_flat = labels.view(-1)\n",
    "#                         active = labels_flat != model.ignore_index\n",
    "#                         true_labels.extend(labels_flat[active].cpu().numpy())\n",
    "#                         predictions.extend(preds[active].cpu().numpy())\n",
    "\n",
    "#                     running_loss += loss.item() * inputs.size(0) if loss is not None else 0\n",
    "\n",
    "#             epoch_loss = running_loss / len(data_loaders[phase].dataset) if loss is not None else 0\n",
    "#             epoch_acc = accuracy_score(true_labels, predictions) if labels is not None else 0\n",
    "\n",
    "#             print(f'{phase.title()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "#             if phase == 'test' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "#         print()\n",
    "\n",
    "#     print(f'Best Test Accuracy: {best_acc:.4f}')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def train(model, data_sets, optimizer, num_epochs: int, batch_size=16):\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     criterion = torch.nn.CrossEntropyLoss(ignore_index=model.ignore_index)\n",
    "#     model.to(device)\n",
    "\n",
    "#     data_loaders = {\n",
    "#         \"train\": DataLoader(data_sets[\"train\"], batch_size=batch_size, shuffle=True),\n",
    "#         \"test\": DataLoader(data_sets[\"test\"], batch_size=batch_size, shuffle=False)\n",
    "#     }\n",
    "\n",
    "#     best_acc = 0.0\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "#         print('-' * 10)\n",
    "\n",
    "#         for phase in ['train', 'test']:\n",
    "#             model.train() if phase == 'train' else model.eval()\n",
    "\n",
    "#             running_loss = 0.0\n",
    "#             total_samples = 0\n",
    "#             correct_predictions = 0\n",
    "\n",
    "#             for batch in data_loaders[phase]:\n",
    "#                 inputs = batch[\"input_ids\"].to(device)\n",
    "#                 labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 with torch.set_grad_enabled(phase == 'train'):\n",
    "#                     outputs, _ = model(inputs)\n",
    "#                     loss = None\n",
    "\n",
    "#                     if phase == 'train':\n",
    "#                         # Flatten logits and labels to compute the loss for training\n",
    "#                         loss = criterion(outputs.view(-1, model.num_classes), labels.view(-1))\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "#                     elif phase == 'test':\n",
    "#                         # No need to flatten for loss computation in test phase\n",
    "#                         loss = criterion(outputs.view(-1, model.num_classes), labels.view(-1))\n",
    "\n",
    "#                     # Calculate accuracy\n",
    "#                     preds = torch.argmax(outputs, dim=2)  # Predictions across num_classes dimension\n",
    "#                     preds = preds.view(-1)  # Flatten predictions\n",
    "#                     labels_flat = labels.view(-1)  # Flatten true labels\n",
    "#                     correct = (preds == labels_flat) & (labels_flat != model.ignore_index)  # Exclude ignore_index in accuracy calculation\n",
    "#                     correct_predictions += correct.sum().item()\n",
    "#                     total_samples += correct.size(0)\n",
    "\n",
    "#                     if loss is not None:\n",
    "#                         running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#             epoch_loss = running_loss / len(data_loaders[phase].dataset)\n",
    "#             epoch_acc = correct_predictions / total_samples\n",
    "\n",
    "#             print(f'{phase.title()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "#             if phase == 'test' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "#         print()\n",
    "\n",
    "#     print(f'Best Test Accuracy: {best_acc:.4f}')\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, data_sets, optimizer, num_epochs: int, batch_size=2):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=model.ignore_index)\n",
    "    model.to(device)\n",
    "\n",
    "    data_loaders = {\n",
    "        \"train\": DataLoader(data_sets[\"train\"], batch_size=batch_size, shuffle=True),\n",
    "        \"test\": DataLoader(data_sets[\"test\"], batch_size=batch_size, shuffle=False)\n",
    "    }\n",
    "\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'test']:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            for batch in data_loaders[phase]:\n",
    "                inputs = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs, _ = model(inputs)\n",
    "                    loss = criterion(outputs.view(-1, model.num_classes), labels.view(-1))\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # Store predictions and true labels for F1 score calculation\n",
    "                    preds = torch.argmax(outputs, dim=2).view(-1)  # Flatten predictions\n",
    "                    labels_flat = labels.view(-1)  # Flatten true labels\n",
    "                    valid_indices = labels_flat != model.ignore_index  # Exclude ignore_index\n",
    "\n",
    "                    valid_preds = preds[valid_indices].cpu().numpy()\n",
    "                    valid_labels = labels_flat[valid_indices].cpu().numpy()\n",
    "\n",
    "                    all_preds.extend(valid_preds)\n",
    "                    all_labels.extend(valid_labels)\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loaders[phase].dataset)\n",
    "            epoch_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "            print(f'{phase.title()} Loss: {epoch_loss:.4f} F1: {epoch_f1:.4f}')\n",
    "\n",
    "            if phase == 'test' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        print()\n",
    "\n",
    "    print(f'Best Test F1: {best_f1:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc44a3b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created train\n",
      "Epoch 1/30\n",
      "----------\n",
      "Train Loss: 0.1425 F1: 0.5134\n",
      "Test Loss: 0.2059 F1: 0.5405\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "Train Loss: 0.1234 F1: 0.6201\n",
      "Test Loss: 0.2007 F1: 0.5507\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "Train Loss: 0.1131 F1: 0.6449\n",
      "Test Loss: 0.2046 F1: 0.5654\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "Train Loss: 0.1052 F1: 0.6760\n",
      "Test Loss: 0.2172 F1: 0.5658\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "Train Loss: 0.0992 F1: 0.6970\n",
      "Test Loss: 0.2163 F1: 0.5702\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "Train Loss: 0.0943 F1: 0.7169\n",
      "Test Loss: 0.2247 F1: 0.5820\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "Train Loss: 0.0895 F1: 0.7339\n",
      "Test Loss: 0.2138 F1: 0.6003\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "Train Loss: 0.0873 F1: 0.7427\n",
      "Test Loss: 0.2331 F1: 0.5631\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "Train Loss: 0.0855 F1: 0.7459\n",
      "Test Loss: 0.2220 F1: 0.6001\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "Train Loss: 0.0825 F1: 0.7531\n",
      "Test Loss: 0.2486 F1: 0.5614\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "Train Loss: 0.0789 F1: 0.7561\n",
      "Test Loss: 0.2472 F1: 0.5557\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "Train Loss: 0.0791 F1: 0.7655\n",
      "Test Loss: 0.2561 F1: 0.5656\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "Train Loss: 0.0759 F1: 0.7732\n",
      "Test Loss: 0.2750 F1: 0.5417\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "Train Loss: 0.0766 F1: 0.7676\n",
      "Test Loss: 0.2580 F1: 0.5816\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "Train Loss: 0.0775 F1: 0.7698\n",
      "Test Loss: 0.2574 F1: 0.5731\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "Train Loss: 0.0758 F1: 0.7710\n",
      "Test Loss: 0.2653 F1: 0.5675\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "Train Loss: 0.0765 F1: 0.7714\n",
      "Test Loss: 0.2791 F1: 0.5436\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "Train Loss: 0.0754 F1: 0.7762\n",
      "Test Loss: 0.2734 F1: 0.5530\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "Train Loss: 0.0738 F1: 0.7797\n",
      "Test Loss: 0.2783 F1: 0.5597\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "Train Loss: 0.0723 F1: 0.7759\n",
      "Test Loss: 0.2956 F1: 0.5420\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "Train Loss: 0.0727 F1: 0.7790\n",
      "Test Loss: 0.2910 F1: 0.5374\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "Train Loss: 0.0724 F1: 0.7785\n",
      "Test Loss: 0.2711 F1: 0.5614\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "Train Loss: 0.0722 F1: 0.7791\n",
      "Test Loss: 0.2919 F1: 0.5527\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "Train Loss: 0.0709 F1: 0.7820\n",
      "Test Loss: 0.2907 F1: 0.5631\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "Train Loss: 0.0703 F1: 0.7774\n",
      "Test Loss: 0.2939 F1: 0.5648\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "Train Loss: 0.0701 F1: 0.7861\n",
      "Test Loss: 0.2890 F1: 0.5531\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "Train Loss: 0.0706 F1: 0.7879\n",
      "Test Loss: 0.3005 F1: 0.5429\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "Train Loss: 0.0708 F1: 0.7863\n",
      "Test Loss: 0.2914 F1: 0.5631\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "Train Loss: 0.0694 F1: 0.7842\n",
      "Test Loss: 0.2974 F1: 0.5652\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "Train Loss: 0.0689 F1: 0.7835\n",
      "Test Loss: 0.2960 F1: 0.5472\n",
      "\n",
      "Best Test F1: 0.6003\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "train_path = '/Users/ranykhirbawi/Desktop/data 2/train.tagged'\n",
    "test_path = '/Users/ranykhirbawi/Desktop/data 2/dev.tagged'\n",
    "\n",
    "train_ds = NERDataSet(train_path, vector_type='glove', untagged=False)\n",
    "print('created train')\n",
    "test_ds = NERDataSet(test_path, vector_type='glove', untagged=False)\n",
    "\n",
    "\n",
    "datasets = {\"train\": train_ds, \"test\": test_ds}\n",
    "FF_model = FF_NN(num_classes=2, vec_dim=train_ds.vector_dim)\n",
    "optimizer = Adam(params=FF_model.parameters())\n",
    "train(model=FF_model, data_sets=datasets, optimizer=optimizer, num_epochs=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ae048a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_classes, num_layers=1, dropout=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.ignore_index = -100\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        # Activation\n",
    "        self.activation = nn.ReLU()\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Loss\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        # LSTM output\n",
    "        lstm_out, _ = self.lstm(input_ids)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # Pass the output of the LSTM layer to the fully connected layer\n",
    "        logits = self.fc(lstm_out)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Flatten logits and labels to compute the loss\n",
    "            logits_flat = logits.view(-1, logits.size(-1))\n",
    "            labels_flat = labels.view(-1)\n",
    "            loss = self.loss(logits_flat, labels_flat)\n",
    "            return logits, loss\n",
    "        \n",
    "        return logits, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a1ab3ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "Train Loss: 0.1453 F1: 0.4781\n",
      "Test Loss: 0.2020 F1: 0.5667\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "Train Loss: 0.1145 F1: 0.6241\n",
      "Test Loss: 0.2022 F1: 0.5959\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "Train Loss: 0.1010 F1: 0.6818\n",
      "Test Loss: 0.2040 F1: 0.5837\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "Train Loss: 0.0878 F1: 0.7214\n",
      "Test Loss: 0.2065 F1: 0.6108\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "Train Loss: 0.0744 F1: 0.7755\n",
      "Test Loss: 0.2137 F1: 0.6147\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "Train Loss: 0.0637 F1: 0.8084\n",
      "Test Loss: 0.2351 F1: 0.5980\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "Train Loss: 0.0564 F1: 0.8377\n",
      "Test Loss: 0.2453 F1: 0.6219\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "Train Loss: 0.0468 F1: 0.8703\n",
      "Test Loss: 0.2593 F1: 0.6210\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "Train Loss: 0.0412 F1: 0.8843\n",
      "Test Loss: 0.2771 F1: 0.6107\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "Train Loss: 0.0363 F1: 0.9027\n",
      "Test Loss: 0.2925 F1: 0.6039\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "Train Loss: 0.0316 F1: 0.9157\n",
      "Test Loss: 0.3281 F1: 0.6028\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "Train Loss: 0.0271 F1: 0.9280\n",
      "Test Loss: 0.3731 F1: 0.5951\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "Train Loss: 0.0267 F1: 0.9253\n",
      "Test Loss: 0.3483 F1: 0.6077\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "Train Loss: 0.0228 F1: 0.9394\n",
      "Test Loss: 0.3749 F1: 0.6073\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "Train Loss: 0.0210 F1: 0.9432\n",
      "Test Loss: 0.4042 F1: 0.5887\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "Train Loss: 0.0195 F1: 0.9500\n",
      "Test Loss: 0.3943 F1: 0.5939\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "Train Loss: 0.0190 F1: 0.9493\n",
      "Test Loss: 0.4449 F1: 0.5897\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "Train Loss: 0.0168 F1: 0.9577\n",
      "Test Loss: 0.4301 F1: 0.5866\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "Train Loss: 0.0165 F1: 0.9551\n",
      "Test Loss: 0.4593 F1: 0.5887\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "Train Loss: 0.0152 F1: 0.9613\n",
      "Test Loss: 0.4571 F1: 0.5913\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "Train Loss: 0.0139 F1: 0.9655\n",
      "Test Loss: 0.4357 F1: 0.6070\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "Train Loss: 0.0154 F1: 0.9603\n",
      "Test Loss: 0.4635 F1: 0.5893\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "Train Loss: 0.0126 F1: 0.9690\n",
      "Test Loss: 0.4629 F1: 0.5970\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "Train Loss: 0.0119 F1: 0.9681\n",
      "Test Loss: 0.5050 F1: 0.5817\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "Train Loss: 0.0126 F1: 0.9680\n",
      "Test Loss: 0.4836 F1: 0.6002\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "Train Loss: 0.0131 F1: 0.9664\n",
      "Test Loss: 0.4891 F1: 0.6045\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "Train Loss: 0.0119 F1: 0.9676\n",
      "Test Loss: 0.5086 F1: 0.5939\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "Train Loss: 0.0124 F1: 0.9666\n",
      "Test Loss: 0.5234 F1: 0.5955\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "Train Loss: 0.0112 F1: 0.9699\n",
      "Test Loss: 0.5231 F1: 0.6077\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "Train Loss: 0.0101 F1: 0.9733\n",
      "Test Loss: 0.5262 F1: 0.6065\n",
      "\n",
      "Best Test F1: 0.6219\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_ds and test_ds are already defined and loaded\n",
    "embedding_dim = 200  # Match this with your embedding dimension\n",
    "hidden_dim = 100\n",
    "num_classes = 2\n",
    "num_layers = 1\n",
    "dropout = 0.5\n",
    "\n",
    "lstm_model = SentimentLSTM(embedding_dim, hidden_dim, num_classes, num_layers, dropout)\n",
    "optimizer = Adam(lstm_model.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(lstm_model, {\"train\": train_ds, \"test\": test_ds}, optimizer, num_epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c58c3e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5cf2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29706a59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c41a3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cbc80e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}